{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Repository ID\n",
    "repo_id = 'google/flan-t5-xxl'\n",
    "\n",
    "# 질의내용\n",
    "question = \"who is Son Heung Min?\"\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# HuggingFaceHub 객체 생성\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    model_kwargs={\"temperature\": 0.2, \n",
    "                  \"max_length\": 512}\n",
    ")\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_chain.run(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Generates a completion for a given prompt using the specified model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for generating the completion.\n",
    "        model (str): The name of the model to use for generating the completion.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated completion text.\n",
    "    \"\"\"\n",
    "    # Create a list of message dictionaries with the user's prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    client = OpenAI()\n",
    "    # Generate a chat completion using OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # Set temperature to 0 for deterministic output\n",
    "    )\n",
    "\n",
    "    # Extract and return the generated completion content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget --output-document=llama-2-7b-chat.bin https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m      5\u001b[0m llm \u001b[38;5;241m=\u001b[39m CTransformers(\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2-7b-chat.ggmlv3.q8_0.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Location of the downloaded GGML model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Specify the model type as Llama\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     },  \u001b[38;5;66;03m# Set specific configuration\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize an LLMChain instance with the provided prompt, llm, and verbose setting\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(prompt\u001b[38;5;241m=\u001b[39m\u001b[43mprompt\u001b[49m, llm\u001b[38;5;241m=\u001b[39mllm, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain import LLMChain\n",
    "\n",
    "# Initialize a Local CTransformers wrapper for the Llama-2-7B-Chat model\n",
    "llm = CTransformers(\n",
    "    model=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",  # Location of the downloaded GGML model\n",
    "    model_type=\"llama\",  # Specify the model type as Llama\n",
    "    config={\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.1,\n",
    "    },  # Set specific configuration\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an LLMChain instance with the provided prompt, llm, and verbose setting\\\n",
    "# 질의내용\n",
    "from langchain.prompts import PromptTemplate\n",
    "question = \"who is Son Heung Min?\"\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\lang\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: who is Son Heung Min?\n",
      "\n",
      "Answer: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Son Heung-min is a South Korean professional footballer who plays as a forward for Premier League club Tottenham Hotspur and the South Korea national team. Born on July 8, 1992, in Chuncheon, South Korea, Son began his football career at the age of 16 with Hamburg SV's youth academy before moving to Bayer Leverkusen in 2009. He made his professional debut for Leverkusen in 2010 and spent three seasons with the club before joining Tottenham Hotspur in 2015.\\n\\nSon has established himself as one of the most promising young players in the world, known for his speed, skill, and goal-scoring ability. He has won numerous awards and accolades throughout his career, including the Premier League Player of the Month award in April 2019 and the FIFA World Cup Dream Team in 2018. Son has also been named to the UEFA Champions League Team of the Season and the Premier League Team of the Year multiple times.\\n\\nInternationally, Son has represented South Korea at various youth levels before making his senior debut in 2013.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(question=question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
