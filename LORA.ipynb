{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\lang\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d400179eadff488cab1774070bb123c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model ID for the sharded FALCON model by vilsonrodrigues\n",
    "model_id = \"vilsonrodrigues/falcon-7b-instruct-sharded\"\n",
    "\n",
    "# Configure BitsAndBytesConfig for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Initialize the tokenizer using the model ID and set the pad token to be the same as the end of sentence token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize the pre-trained model using AutoModelForCausalLM\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "pretrained_model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 3613463424 || trainable%: 0.13058363808693696\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "We'll be using a dataset of Mayo Clinic symptoms and diseases.\n",
    "It contains information about 1,058 rows (symptoms) and related diseases.\n",
    "The file weughts 626 kB\n",
    "\"\"\"\n",
    "\n",
    "data = load_dataset(\"celikmus/mayo_clinic_symptoms_and_diseases_v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 1058\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = data.map(\n",
    "    lambda x: {\n",
    "        \"input_text\": f\"symptoms: {x['text']}; most likely explanation: {x['label']}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_encodings = tokenizer(\n",
    "    train_dataset[\"input_text\"],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        \"\"\"\n",
    "        Initialize a custom dataset for text inputs and encodings.\n",
    "\n",
    "        Parameters:\n",
    "            encodings (dict): A dictionary containing the encoded inputs.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset by index.\n",
    "\n",
    "        Parameters:\n",
    "            idx (int): The index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the encoded input and labels.\n",
    "        \"\"\"\n",
    "        # Create an item dictionary with tensors for each encoding key\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "        # Copy input_ids to labels for tasks like language modeling\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encodings to PyTorch datasets\n",
    "train_dataset = TextDataset(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_text = \"\"\"\n",
    "In crowded places, I feel cold in the tips of my fingers, I sweat with dizziness.\n",
    "What is happening?\n",
    "\"\"\"\n",
    "# hint: agoraphobia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "C:\\Users\\user\\.cache\\huggingface\\modules\\transformers_modules\\vilsonrodrigues\\falcon-7b-instruct-sharded\\0e7ea20c0bfd0665eaf3835f1efd12a0e8f02d90\\modeling_falcon.py:336: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = F.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(request_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "pretrained_model_output = pretrained_model.generate(\n",
    "    input_ids=encoding.input_ids,\n",
    "    attention_mask=encoding.attention_mask,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.25,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'In crowded places, I feel cold in the tips of my fingers, I sweat with '\n",
      " 'dizziness.\\n'\n",
      " 'What is happening?\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone.\\n'\n",
      " 'I am not alone')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(tokenizer.decode(pretrained_model_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_text = \"I started to feel swelling  and itching around the mouth and throat after a salad with peanuts, cherry tomatoes and cheese. What may be the reason?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(request_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "pretrained_model_output = pretrained_model.generate(\n",
    "    input_ids=encoding.input_ids,\n",
    "    attention_mask=encoding.attention_mask,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.25,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.05,\n",
    "        max_steps=40,\n",
    "        learning_rate=2.5e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059fe886685d447180d2c96600db2a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_14432\\2194865541.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1058, 'grad_norm': 0.17878128588199615, 'learning_rate': 0.000125, 'epoch': 0.03}\n",
      "{'loss': 2.0841, 'grad_norm': 0.16140981018543243, 'learning_rate': 0.00025, 'epoch': 0.06}\n",
      "{'loss': 2.2015, 'grad_norm': 0.1694914549589157, 'learning_rate': 0.0002495730616258337, 'epoch': 0.09}\n",
      "{'loss': 2.1433, 'grad_norm': 0.2552121579647064, 'learning_rate': 0.0002482951629253403, 'epoch': 0.12}\n",
      "{'loss': 2.1502, 'grad_norm': 0.30767083168029785, 'learning_rate': 0.0002461750332424163, 'epoch': 0.15}\n",
      "{'loss': 2.0273, 'grad_norm': 0.28413626551628113, 'learning_rate': 0.00024322715521257933, 'epoch': 0.18}\n",
      "{'loss': 1.9846, 'grad_norm': 0.3133431673049927, 'learning_rate': 0.00023947166583188217, 'epoch': 0.21}\n",
      "{'loss': 2.0909, 'grad_norm': 0.37438762187957764, 'learning_rate': 0.00023493421890081112, 'epoch': 0.24}\n",
      "{'loss': 2.049, 'grad_norm': 0.44594240188598633, 'learning_rate': 0.00022964580978281608, 'epoch': 0.27}\n",
      "{'loss': 2.0225, 'grad_norm': 0.5678523778915405, 'learning_rate': 0.00022364256367454923, 'epoch': 0.3}\n",
      "{'loss': 2.0221, 'grad_norm': 0.6497517824172974, 'learning_rate': 0.00021696548883414147, 'epoch': 0.33}\n",
      "{'loss': 1.9864, 'grad_norm': 1.1973469257354736, 'learning_rate': 0.00020966019645321765, 'epoch': 0.36}\n",
      "{'loss': 1.7049, 'grad_norm': 1.0875556468963623, 'learning_rate': 0.0002017765890862085, 'epoch': 0.39}\n",
      "{'loss': 1.895, 'grad_norm': 0.6468470096588135, 'learning_rate': 0.00019336851976530336, 'epoch': 0.42}\n",
      "{'loss': 1.8727, 'grad_norm': 0.6240653991699219, 'learning_rate': 0.0001844934241296342, 'epoch': 0.45}\n",
      "{'loss': 1.88, 'grad_norm': 0.6054245829582214, 'learning_rate': 0.0001752119280816212, 'epoch': 0.48}\n",
      "{'loss': 1.7975, 'grad_norm': 0.5446487665176392, 'learning_rate': 0.00016558743365058546, 'epoch': 0.51}\n",
      "{'loss': 1.8344, 'grad_norm': 0.6123881340026855, 'learning_rate': 0.0001556856858925999, 'epoch': 0.54}\n",
      "{'loss': 1.7485, 'grad_norm': 0.6099372506141663, 'learning_rate': 0.00014557432378509174, 'epoch': 0.57}\n",
      "{'loss': 1.74, 'grad_norm': 0.6177421808242798, 'learning_rate': 0.00013532241818404157, 'epoch': 0.6}\n",
      "{'loss': 1.7055, 'grad_norm': 0.6831942200660706, 'learning_rate': 0.000125, 'epoch': 0.63}\n",
      "{'loss': 1.9096, 'grad_norm': 0.6055781245231628, 'learning_rate': 0.00011467758181595848, 'epoch': 0.66}\n",
      "{'loss': 1.9496, 'grad_norm': 0.6559004783630371, 'learning_rate': 0.00010442567621490824, 'epoch': 0.69}\n",
      "{'loss': 1.6186, 'grad_norm': 0.6857738494873047, 'learning_rate': 9.431431410740012e-05, 'epoch': 0.72}\n",
      "{'loss': 1.8672, 'grad_norm': 0.6644763350486755, 'learning_rate': 8.441256634941458e-05, 'epoch': 0.75}\n",
      "{'loss': 1.7268, 'grad_norm': 0.6397618651390076, 'learning_rate': 7.478807191837885e-05, 'epoch': 0.78}\n",
      "{'loss': 1.853, 'grad_norm': 0.753626823425293, 'learning_rate': 6.55065758703658e-05, 'epoch': 0.81}\n",
      "{'loss': 1.7134, 'grad_norm': 0.6419522166252136, 'learning_rate': 5.663148023469666e-05, 'epoch': 0.84}\n",
      "{'loss': 1.6249, 'grad_norm': 0.7271429896354675, 'learning_rate': 4.822341091379152e-05, 'epoch': 0.87}\n",
      "{'loss': 1.7908, 'grad_norm': 0.7366359233856201, 'learning_rate': 4.033980354678239e-05, 'epoch': 0.9}\n",
      "{'loss': 1.6431, 'grad_norm': 0.7713507413864136, 'learning_rate': 3.303451116585855e-05, 'epoch': 0.93}\n",
      "{'loss': 1.6206, 'grad_norm': 0.6985048651695251, 'learning_rate': 2.635743632545082e-05, 'epoch': 0.96}\n",
      "{'loss': 1.6474, 'grad_norm': 0.6927882432937622, 'learning_rate': 2.0354190217183932e-05, 'epoch': 0.99}\n",
      "{'loss': 1.7703, 'grad_norm': 0.7573891282081604, 'learning_rate': 1.506578109918888e-05, 'epoch': 1.02}\n",
      "{'loss': 1.7736, 'grad_norm': 0.6962332725524902, 'learning_rate': 1.0528334168117812e-05, 'epoch': 1.05}\n",
      "{'loss': 1.7375, 'grad_norm': 0.7015750408172607, 'learning_rate': 6.77284478742067e-06, 'epoch': 1.08}\n",
      "{'loss': 1.7505, 'grad_norm': 0.7535085678100586, 'learning_rate': 3.824966757583703e-06, 'epoch': 1.11}\n",
      "{'loss': 1.7426, 'grad_norm': 0.6791476011276245, 'learning_rate': 1.7048370746597097e-06, 'epoch': 1.14}\n",
      "{'loss': 1.7498, 'grad_norm': 0.6666255593299866, 'learning_rate': 4.2693837416626914e-07, 'epoch': 1.17}\n",
      "{'loss': 1.7417, 'grad_norm': 0.6690855026245117, 'learning_rate': 0.0, 'epoch': 1.2}\n",
      "{'train_runtime': 380.6954, 'train_samples_per_second': 3.362, 'train_steps_per_second': 0.105, 'train_loss': 1.8569317221641541, 'epoch': 1.2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=1.8569317221641541, metrics={'train_runtime': 380.6954, 'train_samples_per_second': 3.362, 'train_steps_per_second': 0.105, 'total_flos': 1.297590694772736e+16, 'train_loss': 1.8569317221641541, 'epoch': 1.2030075187969924})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model:\n",
    "trained_model = (\n",
    "    trainer.model.module if hasattr(trainer.model, \"module\") else trainer.model\n",
    ")  # Take care of distributed/parallel training\n",
    "trained_model.save_pretrained(\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "# Now we can inference our model:\n",
    "lora_config = LoraConfig.from_pretrained(\"outputs\")\n",
    "loaded_model = get_peft_model(\n",
    "    prepare_model_for_kbit_training(pretrained_model), lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): FalconForCausalLM(\n",
       "      (transformer): FalconModel(\n",
       "        (word_embeddings): Embedding(65024, 4544)\n",
       "        (h): ModuleList(\n",
       "          (0-31): 32 x FalconDecoderLayer(\n",
       "            (self_attention): FalconAttention(\n",
       "              (maybe_rotary): FalconRotaryEmbedding()\n",
       "              (query_key_value): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4544, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4672, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): FalconMLP(\n",
       "              (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.config.use_cache = True\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15317"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del trained_model\n",
    "del trainer\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e84ebc1f57c4a7890dc0b4cad6c7d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = (\n",
    "    \"vilsonrodrigues/falcon-7b-instruct-sharded\"  # sharded model by vilsonrodrigues\n",
    ")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device_map={\"\": 0}, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query):\n",
    "    \"\"\"\n",
    "    Generate responses from both the original model and PEFT model and compare their answers.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The user query for which responses are generated.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # System and user prompts\n",
    "    system_prompt = \"\"\"Answer the following question truthfully.\n",
    "          If you don't know the answer or the question is too complex,\n",
    "          respond 'Kindly, consult a doctor for further queries.'.\"\"\"\n",
    "    user_prompt = f\"\"\"<HUMAN>: {query}\n",
    "      <ASSISTANT>: \"\"\"\n",
    "    final_prompt = system_prompt + \"\\n\" + user_prompt\n",
    "\n",
    "    # Device and dashline\n",
    "    device = \"cuda:0\"\n",
    "    dashline = \"-\" * 50\n",
    "\n",
    "    # Encode prompt and generate response from the original model\n",
    "    encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    output = pretrained_model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.25,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    text_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print original model response\n",
    "    pprint.pprint(dashline)\n",
    "    pprint.pprint(f\"ORIGINAL MODEL RESPONSE:\\n{text_output}\")\n",
    "    pprint.pprint(dashline)\n",
    "\n",
    "    # Encode prompt and generate response from the PEFT model\n",
    "    peft_encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    peft_output = loaded_model.generate(\n",
    "        input_ids=peft_encoding.input_ids,\n",
    "        attention_mask=peft_encoding.attention_mask,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.25,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    peft_text_output = tokenizer.decode(peft_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print PEFT model response\n",
    "    pprint.pprint(f\"PEFT MODEL RESPONSE:\\n{peft_text_output}\")\n",
    "    pprint.pprint(dashline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'--------------------------------------------------'\n",
      "('ORIGINAL MODEL RESPONSE:\\n'\n",
      " 'Answer the following question truthfully.\\n'\n",
      " \"          If you don't know the answer or the question is too complex,\\n\"\n",
      " \"          respond 'Kindly, consult a doctor for further queries.'.\\n\"\n",
      " '<HUMAN>: In crowded places, I feel cold in the tips of my fingers, I sweat '\n",
      " 'with dizziness. What may be happening?\\n'\n",
      " '      <ASSISTANT>: <HUMAN> may be experiencing symptoms of hypothermia. It '\n",
      " 'is advised to seek medical attention immediately.')\n",
      "'--------------------------------------------------'\n",
      "('PEFT MODEL RESPONSE:\\n'\n",
      " 'Answer the following question truthfully.\\n'\n",
      " \"          If you don't know the answer or the question is too complex,\\n\"\n",
      " \"          respond 'Kindly, consult a doctor for further queries.'.\\n\"\n",
      " '<HUMAN>: In crowded places, I feel cold in the tips of my fingers, I sweat '\n",
      " 'with dizziness. What may be happening?\\n'\n",
      " '      <ASSISTANT>: <HUMAN> may be experiencing symptoms of hypothermia. It '\n",
      " 'is recommended to seek medical attention immediately.')\n",
      "'--------------------------------------------------'\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"In crowded places, I feel cold in the tips of my fingers, I sweat with dizziness. What may be happening?\"\"\"\n",
    "generate_answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'--------------------------------------------------'\n",
      "('ORIGINAL MODEL RESPONSE:\\n'\n",
      " 'Answer the following question truthfully.\\n'\n",
      " \"          If you don't know the answer or the question is too complex,\\n\"\n",
      " \"          respond 'Kindly, consult a doctor for further queries.'.\\n\"\n",
      " '<HUMAN>: I started to feel swelling  and itching around the mouth and throat '\n",
      " 'after a salad with peanuts, cherry tomatoes and cheese. What may be the '\n",
      " 'reason?\\n'\n",
      " \"      <ASSISTANT>:  It's possible that the swelling and itching are due to \"\n",
      " 'an allergic reaction to peanuts. Have you had any previous allergic '\n",
      " 'reactions to peanuts or similar foods?')\n",
      "'--------------------------------------------------'\n",
      "('PEFT MODEL RESPONSE:\\n'\n",
      " 'Answer the following question truthfully.\\n'\n",
      " \"          If you don't know the answer or the question is too complex,\\n\"\n",
      " \"          respond 'Kindly, consult a doctor for further queries.'.\\n\"\n",
      " '<HUMAN>: I started to feel swelling  and itching around the mouth and throat '\n",
      " 'after a salad with peanuts, cherry tomatoes and cheese. What may be the '\n",
      " 'reason?\\n'\n",
      " '      <ASSISTANT>:  It is possible that the swelling and itching are due to '\n",
      " 'an allergic reaction to the peanuts, cherry tomatoes or cheese. Kindly '\n",
      " 'consult a doctor for further queries.')\n",
      "'--------------------------------------------------'\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"I started to feel swelling  and itching around the mouth and throat after a salad with peanuts, cherry tomatoes and cheese. What may be the reason?\"\"\"\n",
    "generate_answer(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
